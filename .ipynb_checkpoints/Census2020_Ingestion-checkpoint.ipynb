{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import datetime\n",
    "from datetime import timedelta,date\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark\n",
    "import psycopg2\n",
    "import sys\n",
    "from delta.tables import *\n",
    "import pandas as pd\n",
    "import urllib \n",
    "import io\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "import google.cloud\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'District_of_Columbia', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New_Hampshire', 'New_Jersey', 'New_Mexico', 'New_York', 'North_Carolina', 'North_Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Puerto_Rico', 'Rhode_Island', 'South_Carolina', 'South_Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West_Virginia', 'Wisconsin', 'Wyoming']\n",
    "us_states_abb = ['al', 'ak', 'az', 'ar', 'ca', 'co', 'ct', 'de', 'dc', 'fl', 'ga', 'hi', 'id', 'il', 'in', 'ia', 'ks', 'ky', 'la', 'me', 'md', 'ma', 'mi', 'mn', 'ms', 'mo', 'mt', 'ne', 'nv', 'nh', 'nj', 'nm', 'ny', 'nc', 'nd', 'oh', 'ok', 'or', 'pa', 'pr', 'ri', 'sc', 'sd', 'tn', 'tx', 'ut', 'vt', 'va', 'wa', 'wv', 'wi', 'wy']\n",
    "full_states_tuple = list(zip(us_states, us_states_abb))\n",
    "\n",
    "zip_file = '2020.pl.zip'\n",
    "\n",
    "ftp_index_url = 'https://www2.census.gov/programs-surveys/decennial/2020/data/01-Redistricting_File--PL_94-171/'\n",
    "\n",
    "dbutils.fs.mkdirs(\"/dbfs/census20_tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything has to be read as a string - too many columns to specify schema individually\n",
    "# only return pl_1 and geo because OOM otherwise\n",
    "def extract_zip(input_zips_list, us_segment, section = '12020.pl'):\n",
    "    pl = []\n",
    "    for location in input_zips_list:\n",
    "        tmp_location = ZipFile(location)\n",
    "        for name in tmp_location.namelist():\n",
    "            if section in name:\n",
    "                pl_file = tmp_location.read(name)\n",
    "                pl_df = pd.read_table(io.StringIO(pl_file.decode('ISO-8859-1')), \n",
    "                                      delimiter = \"|\", \n",
    "                                      header = None, \n",
    "                                      dtype = str)\n",
    "                print(name)\n",
    "                pl.append(pl_df)\n",
    "        pl_df = pd.concat(pl, ignore_index = True)\n",
    "        pl_df.to_csv(\"/dbfs/census20_tmp/\" + us_segment + \"_\" + section[0:5] + \".csv\")\n",
    "        return\n",
    "\n",
    "def dataset_check_or_create(dataset_name):\n",
    "    key_path = \"/dbfs/FileStore/tables/fidap_301014_de8cdbcaf7a0.json\"\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "  )\n",
    "\n",
    "    client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "    \n",
    "    dataset_id = credentials.project_id + \".\" + dataset_name\n",
    "    try:\n",
    "        client.get_dataset(dataset_id)  # Make an API request.\n",
    "        print(\"Dataset {} already exists\".format(dataset_id))\n",
    "        return True\n",
    "    except Exception:\n",
    "        print(\"Dataset {} is not found\".format(dataset_id))\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        # TODO(developer): Specify the geographic location where the dataset should reside.\n",
    "        dataset.location = \"US\"\n",
    "        dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "        return False\n",
    "\n",
    "def write_to_bq_new(df , dataset_name, table_name, mode = \"overwrite\"):\n",
    "    print(f\"Writing {table_name} with {mode} - mode into BigQuery \")\n",
    "    dataset_check_or_create(dataset_name)\n",
    "    \n",
    "    df.write \\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"credentialsFile\", \"/dbfs/FileStore/tables/fidap_301014_de8cdbcaf7a0.json\")\\\n",
    "    .option(\"temporaryGcsBucket\", \"databricks-fidap\")\\\n",
    "    .option(\"parentProject\", 'fidap-301014')\\\n",
    "    .option(\"dataset\", dataset_name)\\\n",
    "    .option(\"table\", table_name)\\\n",
    "    .mode(mode).save(table_name)\n",
    "\n",
    "    print(f\"Successfully saved {table_name} with {mode} - mode into BigQuery \")\n",
    "    \n",
    "def add_dataset_from_csv(csv, dataset_schema_name, table_name, is_csv_pyspark_df = False):\n",
    "    \"\"\"\n",
    "    reads a data from csv and creates a dataset in bq\n",
    "    required params: csv\n",
    "    \"\"\"\n",
    "\n",
    "    if is_csv_pyspark_df == False:\n",
    "        pyspark_df= spark.createDataFrame(csv)\n",
    "    else:\n",
    "        pyspark_df = csv\n",
    "        \n",
    "    #write to big query\n",
    "    write_to_bq_new(pyspark_df, dataset_schema_name, table_name, mode = \"overwrite\")\n",
    "    # dbutils.fs.rm(\"dbfs:/tmp/{}\".format(file_name))\n",
    "  \n",
    "    return display(pyspark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_location = []\n",
    "for states_abb in us_states_abb:\n",
    "    zip_location = \"/dbfs/census20_tmp/{}\".format(states_abb + \"2020.pl.zip\")\n",
    "    tmp_location.append(zip_location)\n",
    "\n",
    "# extract the files into dfs  \n",
    "extract_zip(tmp_location, us_segment = \"us_states_abb\", section = '12020.pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "all_pl_1_filepath = '/census20_tmp/us_states_abb_12020.csv'\n",
    "pl_1_df = spark.read.format(\"csv\") \\\n",
    "   .option(\"header\", \"true\") \\\n",
    "   .option(\"sep\",\",\") \\\n",
    "   .load(all_pl_1_filepath)\n",
    "pl_1_df = pl_1_df.drop('_c0')\n",
    "pl_1_df.count()\n",
    "\n",
    "all_pl_2_filepath = '/census20_tmp/*22020.csv'\n",
    "pl_2_df = spark.read.format(\"csv\") \\\n",
    "   .option(\"header\", \"true\") \\\n",
    "   .option(\"sep\",\",\") \\\n",
    "   .load(all_pl_2_filepath)\n",
    "pl_2_df = pl_2_df.drop('_c0')\n",
    "\n",
    "all_pl_3_filepath = '/census20_tmp/*32020.csv'\n",
    "pl_3_df = spark.read.format(\"csv\") \\\n",
    "   .option(\"header\", \"true\") \\\n",
    "   .option(\"sep\",\",\") \\\n",
    "   .load(all_pl_3_filepath)\n",
    "pl_3_df = pl_3_df.drop('_c0')\n",
    "\n",
    "geo_filepath = '/census20_tmp/us_*_geo.csv'\n",
    "geo_df = spark.read.format(\"csv\") \\\n",
    "   .option(\"header\", \"true\") \\\n",
    "   .option(\"sep\",\",\") \\\n",
    "   .load(geo_filepath)\n",
    "geo_df = geo_df.drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names\n",
    "geo_cols = [\"FILEID\", \"STUSAB\", \"SUMLEV\", \"GEOVAR\", \"GEOCOMP\", \"CHARITER\", \"CIFSN\", \"LOGRECNO\", \"GEOID\", \n",
    "  \"GEOCODE\", \"REGION\", \"DIVISION\", \"STATE\", \"STATENS\", \"COUNTY\", \"COUNTYCC\", \"COUNTYNS\", \"COUSUB\",\n",
    "  \"COUSUBCC\", \"COUSUBNS\", \"SUBMCD\", \"SUBMCDCC\", \"SUBMCDNS\", \"ESTATE\", \"ESTATECC\", \"ESTATENS\", \n",
    "  \"CONCIT\", \"CONCITCC\", \"CONCITNS\", \"PLACE\", \"PLACECC\", \"PLACENS\", \"TRACT\", \"BLKGRP\", \"BLOCK\", \n",
    "  \"AIANHH\", \"AIHHTLI\", \"AIANHHFP\", \"AIANHHCC\", \"AIANHHNS\", \"AITS\", \"AITSFP\", \"AITSCC\", \"AITSNS\",\n",
    "  \"TTRACT\", \"TBLKGRP\", \"ANRC\", \"ANRCCC\", \"ANRCNS\", \"CBSA\", \"MEMI\", \"CSA\", \"METDIV\", \"NECTA\",\n",
    "  \"NMEMI\", \"CNECTA\", \"NECTADIV\", \"CBSAPCI\", \"NECTAPCI\", \"UA\", \"UATYPE\", \"UR\", \"CD116\", \"CD118\",\n",
    "  \"CD119\", \"CD120\", \"CD121\", \"SLDU18\", \"SLDU22\", \"SLDU24\", \"SLDU26\", \"SLDU28\", \"SLDL18\", \"SLDL22\",\n",
    "  \"SLDL24\", \"SLDL26\", \"SLDL28\", \"VTD\", \"VTDI\", \"ZCTA\", \"SDELM\", \"SDSEC\", \"SDUNI\", \"PUMA\", \"AREALAND\",\n",
    "  \"AREAWATR\", \"BASENAME\", \"NAME\", \"FUNCSTAT\", \"GCUNI\", \"POP100\", \"HU100\", \"INTPTLAT\", \"INTPTLON\", \n",
    "  \"LSADC\", \"PARTFLAG\", \"UGA\"]\n",
    "\n",
    "first_cols = [\"FILEID\", \"STUSAB\", \"CHARITER\", \"CIFSN\", \"LOGRECNO\"]\n",
    "suffix_pl1_1 = ['P00' + str(i) for i in range(10001, 10072)]\n",
    "suffix_pl1_2 = ['P00' + str(i) for i in range(20001, 20074)]\n",
    "suffix_pl2_1 = ['P00' + str(i) for i in range(30001, 30072)]\n",
    "suffix_pl2_2 = ['P00' + str(i) for i in range(40001, 40074)]\n",
    "suffix_pl2_3 = ['H00' + str(i) for i in range(10001, 10004)]\n",
    "suffix_pl3_1 = ['P00' + str(i) for i in range(50001, 50011)]\n",
    "\n",
    "\n",
    "pl_1_cols = first_cols + suffix_pl1_1 + suffix_pl1_2\n",
    "pl_2_cols = first_cols + suffix_pl2_1 + suffix_pl2_2 + suffix_pl2_3\n",
    "pl_3_cols = first_cols + suffix_pl3_1\n",
    "\n",
    "# rename the columns\n",
    "pl_1_df = pl_1_df.toDF(*pl_1_cols)\n",
    "pl_2_df = pl_2_df.toDF(*pl_2_cols)\n",
    "pl_3_df = pl_3_df.toDF(*pl_3_cols)\n",
    "geo_df = geo_df.toDF(*geo_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = []\n",
    "headers_census_bureau_pl_1 = pd.read_excel(\"https://www2.census.gov/programs-surveys/decennial/rdo/about/2020-census-program/Phase3/SupportMaterials/2020_PLSummaryFile_FieldNames.xlsx\", sheet_name = \"2020 P.L. Segment 1 Definitions\", usecols = 'A,B')\n",
    "headers_census_bureau_pl_1.columns = ['' + cols for cols in ['FIELD', 'DATA_DICTIONARY_REFERENCE']]\n",
    "headers.append(headers_census_bureau_pl_1)\n",
    "headers_census_bureau_pl_2 = pd.read_excel(\"https://www2.census.gov/programs-surveys/decennial/rdo/about/2020-census-program/Phase3/SupportMaterials/2020_PLSummaryFile_FieldNames.xlsx\", sheet_name = \"2020 P.L. Segment 2 Definitions\", usecols = 'A,B')\n",
    "headers_census_bureau_pl_2.columns = ['' + cols for cols in ['FIELD', 'DATA_DICTIONARY_REFERENCE']]\n",
    "headers.append(headers_census_bureau_pl_2)\n",
    "headers_census_bureau_pl_3 = pd.read_excel(\"https://www2.census.gov/programs-surveys/decennial/rdo/about/2020-census-program/Phase3/SupportMaterials/2020_PLSummaryFile_FieldNames.xlsx\", sheet_name = \"2020 P.L. Segment 3 Definitions\", usecols = 'A,B')\n",
    "headers_census_bureau_pl_3.columns = ['' + cols for cols in ['FIELD', 'DATA_DICTIONARY_REFERENCE']]\n",
    "headers.append(headers_census_bureau_pl_3)\n",
    "headers_census_bureau_geo = pd.read_excel(\"https://www2.census.gov/programs-surveys/decennial/rdo/about/2020-census-program/Phase3/SupportMaterials/2020_PLSummaryFile_FieldNames.xlsx\", sheet_name = \"2020 P.L. Geoheader Definitions\", usecols = 'A,B')\n",
    "headers_census_bureau_geo.columns = ['' + cols for cols in ['FIELD', 'DATA_DICTIONARY_REFERENCE']]\n",
    "headers.append(headers_census_bureau_geo)\n",
    "\n",
    "all_headers = pd.concat(headers, ignore_index = True)\n",
    "all_headers = all_headers.dropna()\n",
    "all_headers = all_headers.drop_duplicates()\n",
    "\n",
    "\n",
    "p = re.compile(r'[^\\w\\s]+')\n",
    "all_headers['FIELD'] = [p.sub('', x) for x in all_headers['FIELD'].tolist()]\n",
    "all_headers['DATA_DICTIONARY_REFERENCE'] = [p.sub('', x) for x in all_headers['DATA_DICTIONARY_REFERENCE'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove certain columns before merging\n",
    "geo_df = geo_df.drop('CIFSN')\n",
    "pl_1_df = pl_1_df.drop('CIFSN')\n",
    "pl_2_df = pl_2_df.drop('CIFSN')\n",
    "pl_3_df = pl_3_df.drop('CIFSN')\n",
    "\n",
    "# writing to dbfs first\n",
    "#geo_df.write.csv('/census20_tmp/census2020_geo.csv')\n",
    "#pl_1_df.write.csv('/census20_tmp/census2020_pl_1.csv')\n",
    "#pl_2_df.write.csv('/census20_tmp/census2020_pl_2.csv')\n",
    "#pl_3_df.write.csv('/census20_tmp/census2020_pl_3.csv')\n",
    "\n",
    "# merge dfs together\n",
    "merged_df = geo_df.join(pl_1_df, on = [\"STUSAB\", \"CHARITER\", \"FILEID\", \"LOGRECNO\"], how = 'inner')\n",
    "merged_df_all = merged_df.join(pl_2_df, on = [\"STUSAB\", \"CHARITER\", \"FILEID\", \"LOGRECNO\"], how = 'inner')\n",
    "merged_df_all = merged_df.join(pl_3_df, on = [\"STUSAB\", \"CHARITER\", \"FILEID\", \"LOGRECNO\"], how = 'inner')\n",
    "\n",
    "# filter for geovar\n",
    "merged_df_all = merged_df_all.filter(merged_df_all.GEOVAR == '00')\n",
    "\n",
    "# count number of rows\n",
    "merged_df_all.count()\n",
    "\n",
    "# filter census\n",
    "census_tract_df = merged_df.filter(size_(col(\"GEOID\")) == 20)\n",
    "census_tract_df = census_tract_df.filter(\"TRACT IS NOT NULL AND BLOCK IS NULL AND BLKGRP IS NULL\")\n",
    "columns = ['GEOID', 'STATE', 'AREALAND', 'HU100', 'P0010001', 'P0010003',  'P0010004', 'P0010005', 'P0010006',  'P0010007', 'P0010008', 'P0010009',  'P0020002', 'P0020003']\n",
    "census_tract_df = census_tract_df.select(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
